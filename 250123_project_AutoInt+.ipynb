{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299ca6e5-28ff-4226-844e-31cfcc1760e0",
   "metadata": {},
   "source": [
    "# AutoInt+ 구현 (with TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "886036a4-726d-4505-b6c9-262d9cd65313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "plt.rc('font', family='NanumMyeongjo')\n",
    "\n",
    "import joblib\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, MaxPooling2D, Conv2D, Dropout, Lambda, Dense, Flatten, Activation, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_normal, Zeros, TruncatedNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d75e34-ad97-46e4-9f3c-136564f87e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(Layer):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "                sum(field_dims), \n",
    "                embed_dim, \n",
    "                embeddings_initializer='glorot_uniform'  # TensorFlow's equivalent of xavier_uniform\n",
    "        )\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = x + tf.constant(self.offsets, dtype=x.dtype)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cb5841-c09b-43ad-8ee8-68c59ef50fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(Layer):\n",
    "    def __init__(self, inputs_dim, hidden_units, activation='relu', l2_reg=0, \n",
    "                 dropout_rate=0, use_bn=False, init_std=0.0001, output_layer=True):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "        if output_layer:\n",
    "            hidden_units += [1]\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(len(hidden_units) - 1):\n",
    "            # Linear layer\n",
    "            layer = Dense(hidden_units[i+1], \n",
    "                          kernel_initializer=tf.random_normal_initializer(mean=0, stddev=init_std),\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(l2_reg))\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "            # Batch Normalization\n",
    "            if use_bn:\n",
    "                self.layers.append(tf.keras.layers.BatchNormalization())\n",
    "            \n",
    "            # Activation\n",
    "            self.layers.append(tf.keras.layers.Activation('relu'))\n",
    "            \n",
    "            # Dropout\n",
    "            self.layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a03dd4-f608-48b5-9b2e-9c9a599681a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIntMLP(Layer):\n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, \n",
    "                 att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, \n",
    "                 dnn_dropout=0.4, init_std=0.0001):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = FeaturesEmbedding(field_dims, embedding_size)\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.att_output_dim = self.num_fields * self.embedding_size\n",
    "        self.embed_output_dim = len(field_dims) * embedding_size\n",
    "        \n",
    "        self.dnn_linear = Dense(1, use_bias=False, \n",
    "                                kernel_initializer=tf.random_normal_initializer(stddev=init_std))\n",
    "        \n",
    "        self.dnn = MultiLayerPerceptron(\n",
    "            self.embed_output_dim, \n",
    "            dnn_hidden_units,\n",
    "            activation=dnn_activation,\n",
    "            l2_reg=l2_reg_dnn,\n",
    "            dropout_rate=dnn_dropout,\n",
    "            use_bn=dnn_use_bn,\n",
    "            init_std=init_std\n",
    "        )\n",
    "        \n",
    "        self.int_layers = [\n",
    "            MultiHeadSelfAttention(\n",
    "                self.embedding_size, \n",
    "                head_num=att_head_num, \n",
    "                use_res=att_res\n",
    "            ) for _ in range(att_layer_num)\n",
    "        ]\n",
    "    \n",
    "    def call(self, X, training=False):\n",
    "        embed_x = self.embedding(X)\n",
    "        dnn_embed = embed_x\n",
    "        att_input = embed_x\n",
    "        \n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "        \n",
    "        att_output = tf.reshape(att_input, [-1, self.att_output_dim])\n",
    "        att_output = tf.nn.relu(self.dnn_linear(att_output))\n",
    "        \n",
    "        dnn_output = self.dnn(tf.reshape(dnn_embed, [-1, self.embed_output_dim]), training=training)\n",
    "        \n",
    "        y_pred = tf.sigmoid(att_output + dnn_output)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed57acc-8f08-4925-9a67-c8db194d72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embedding_size, head_num=2, use_res=True, scaling=False):\n",
    "        super().__init__()\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        if embedding_size % head_num != 0:\n",
    "            raise ValueError('embedding_size is not an integer multiple of head_num!')\n",
    "        \n",
    "        self.att_embedding_size = embedding_size // head_num\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.scaling = scaling\n",
    "        \n",
    "        self.W_Query = tf.Variable(tf.random.normal((embedding_size, embedding_size), stddev=0.05))\n",
    "        self.W_Key = tf.Variable(tf.random.normal((embedding_size, embedding_size), stddev=0.05))\n",
    "        self.W_Value = tf.Variable(tf.random.normal((embedding_size, embedding_size), stddev=0.05))\n",
    "        \n",
    "        if self.use_res:\n",
    "            self.W_Res = tf.Variable(tf.random.normal((embedding_size, embedding_size), stddev=0.05))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) != 3:\n",
    "            raise ValueError(f\"Unexpected inputs dimensions {len(inputs.shape)}, expect to be 3 dimensions\")\n",
    "        \n",
    "        # Linear transformations\n",
    "        querys = tf.tensordot(inputs, self.W_Query, axes=1)\n",
    "        keys = tf.tensordot(inputs, self.W_Key, axes=1)\n",
    "        values = tf.tensordot(inputs, self.W_Value, axes=1)\n",
    "        \n",
    "        # Split heads\n",
    "        querys = tf.stack(tf.split(querys, self.head_num, axis=-1))\n",
    "        keys = tf.stack(tf.split(keys, self.head_num, axis=-1))\n",
    "        values = tf.stack(tf.split(values, self.head_num, axis=-1))\n",
    "        \n",
    "        # Attention\n",
    "        inner_product = tf.einsum('bnik,bnjk->bnij', querys, keys)\n",
    "        if self.scaling:\n",
    "            inner_product /= tf.sqrt(float(self.att_embedding_size))\n",
    "        \n",
    "        normalized_att_scores = tf.nn.softmax(inner_product, axis=-1)\n",
    "        result = tf.matmul(normalized_att_scores, values)\n",
    "        \n",
    "        # Combine heads\n",
    "        result = tf.concat(tf.unstack(result), axis=-1)\n",
    "        \n",
    "        # Residual connection\n",
    "        if self.use_res:\n",
    "            result += tf.tensordot(inputs, self.W_Res, axes=1)\n",
    "        \n",
    "        return tf.nn.relu(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e23742-fa4b-437b-a88f-7c53fb2e8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIntMLPModel(tf.keras.Model):\n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, \n",
    "                 att_res=True, l2_reg_dnn=0, l2_reg_embedding=1e-5, \n",
    "                 dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 dnn_use_bn=False, dnn_dropout=0, init_std=0.0001):\n",
    "        super().__init__()\n",
    "        self.autoInt_mlp_layer = AutoIntMLP(\n",
    "            field_dims, \n",
    "            embedding_size, \n",
    "            att_layer_num=att_layer_num, \n",
    "            att_head_num=att_head_num,\n",
    "            att_res=att_res, \n",
    "            dnn_hidden_units=dnn_hidden_units,\n",
    "            dnn_activation=dnn_activation,\n",
    "            l2_reg_dnn=l2_reg_dnn, \n",
    "            l2_reg_embedding=l2_reg_embedding,\n",
    "            dnn_use_bn=dnn_use_bn, \n",
    "            dnn_dropout=dnn_dropout, \n",
    "            init_std=init_std\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        return self.autoInt_mlp_layer(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aaee5dc-a377-4f6c-80e2-512940a6ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수는 아래의 링크에서 가져왔습니다.\n",
    "# https://www.programcreek.com/python/?code=MaurizioFD%2FRecSys2019_DeepLearning_Evaluation%2FRecSys2019_DeepLearning_Evaluation-master%2FConferences%2FKDD%2FMCRec_our_interface%2FMCRecRecommenderWrapper.py\n",
    "def get_DCG(ranklist, y_true):\n",
    "    dcg = 0.0\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item in y_true:\n",
    "            dcg += 1.0 / math.log(i + 2)\n",
    "    return  dcg\n",
    "\n",
    "def get_IDCG(ranklist, y_true):\n",
    "    idcg = 0.0\n",
    "    i = 0\n",
    "    for item in y_true:\n",
    "        if item in ranklist:\n",
    "            idcg += 1.0 / math.log(i + 2)\n",
    "            i += 1\n",
    "    return idcg\n",
    "\n",
    "def get_NDCG(ranklist, y_true):\n",
    "    '''NDCG 평가 지표'''\n",
    "    ranklist = np.array(ranklist).astype(int)\n",
    "    y_true = np.array(y_true).astype(int)\n",
    "    dcg = get_DCG(ranklist, y_true)\n",
    "    idcg = get_IDCG(y_true, y_true)\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    return round( (dcg / idcg), 5)\n",
    "\n",
    "def get_hit_rate(ranklist, y_true):\n",
    "    '''hitrate 평가 지표'''\n",
    "    c = 0\n",
    "    for y in y_true:\n",
    "        if y in ranklist:\n",
    "            c += 1\n",
    "    return round( c / len(y_true), 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58096d8-f95c-4982-81ef-481a5f295990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_df):\n",
    "    '''모델 테스트'''\n",
    "    user_pred_info = defaultdict(list)\n",
    "    total_rows = len(test_df)\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        features = test_df.iloc[i:i + batch_size, :-1].values\n",
    "        y_pred = model.predict(features, verbose=False)\n",
    "        for feature, p in zip(features, y_pred):\n",
    "            u_i = feature[:2]\n",
    "            user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
    "    return user_pred_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4874a2e-b9ba-4650-9b2a-a10235ed083a",
   "metadata": {},
   "source": [
    "### 데이터 로드 후 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c36e3e-100d-4117-8c2a-9102d63acbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./data/ml-1m/ml-1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfd7110-7d64-4526-a970-026cae60f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_decade</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>rating_month</th>\n",
       "      <th>rating_decade</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1970s</td>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>1990s</td>\n",
       "      <td>1996</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>1960s</td>\n",
       "      <td>1964</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Musical</td>\n",
       "      <td>Romance</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>2000s</td>\n",
       "      <td>2000</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>1990s</td>\n",
       "      <td>1998</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id movie_id movie_decade movie_year rating_year rating_month  \\\n",
       "0       1     1193        1970s       1975        2001            1   \n",
       "1       1      661        1990s       1996        2001            1   \n",
       "2       1      914        1960s       1964        2001            1   \n",
       "3       1     3408        2000s       2000        2001            1   \n",
       "4       1     2355        1990s       1998        2001            1   \n",
       "\n",
       "  rating_decade     genre1      genre2   genre3 gender age occupation    zip  \\\n",
       "0         2000s      Drama          no       no      F   1         10  48067   \n",
       "1         2000s  Animation  Children's  Musical      F   1         10  48067   \n",
       "2         2000s    Musical     Romance       no      F   1         10  48067   \n",
       "3         2000s      Drama          no       no      F   1         10  48067   \n",
       "4         2000s  Animation  Children's   Comedy      F   1         10  48067   \n",
       "\n",
       "  label  \n",
       "0     1  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "# csv 데이터이므로 read_csv로 가져옵니다.\n",
    "movielens_rcmm = pd.read_csv(f\"{data_path}/movielens_rcmm_v2.csv\", dtype=str)\n",
    "print(movielens_rcmm.shape)\n",
    "movielens_rcmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402fd7c7-eaad-4e25-a672-be8cfffcb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {col: LabelEncoder() for col in movielens_rcmm.columns[:-1]} # label은 제외\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    movielens_rcmm[col] = le.fit_transform(movielens_rcmm[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad405fee-47fb-4313-bf98-cbd6efefa388",
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_rcmm['label'] = movielens_rcmm['label'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfdec7bd-14c1-45f8-b203-fcf73e6b5711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 800167 entries, 416292 to 121958\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   user_id        800167 non-null  int32  \n",
      " 1   movie_id       800167 non-null  int32  \n",
      " 2   movie_decade   800167 non-null  int32  \n",
      " 3   movie_year     800167 non-null  int32  \n",
      " 4   rating_year    800167 non-null  int32  \n",
      " 5   rating_month   800167 non-null  int32  \n",
      " 6   rating_decade  800167 non-null  int32  \n",
      " 7   genre1         800167 non-null  int32  \n",
      " 8   genre2         800167 non-null  int32  \n",
      " 9   genre3         800167 non-null  int32  \n",
      " 10  gender         800167 non-null  int32  \n",
      " 11  age            800167 non-null  int32  \n",
      " 12  occupation     800167 non-null  int32  \n",
      " 13  zip            800167 non-null  int32  \n",
      " 14  label          800167 non-null  float32\n",
      "dtypes: float32(1), int32(14)\n",
      "memory usage: 51.9 MB\n"
     ]
    }
   ],
   "source": [
    "# 3. 학습 데이터와 테스트데이터로 분리, 0.2 정도로 분리\n",
    "train_df, test_df = train_test_split(movielens_rcmm, test_size=0.2, random_state=42)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "447541ee-00a1-4713-8a73-eb979c9887be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6040, 3706,   10,   81,    4,   12,    1,   18,   18,   16,    2,\n",
       "          7,   21, 3439], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요 컬럼들과 레이블 정의\n",
    "# 필드의 각 고유 개수를 정의하는 field_dims를 정의합니다. 이는  임베딩 때 활용됩니다. \n",
    "u_i_feature = ['user_id', 'movie_id']\n",
    "meta_features = ['movie_decade', 'movie_year', 'rating_year', 'rating_month', 'rating_decade', 'genre1','genre2', 'genre3', 'gender', 'age', 'occupation', 'zip']\n",
    "label = 'label'\n",
    "field_dims = np.max(movielens_rcmm[u_i_feature + meta_features].astype(np.int64).values, axis=0) + 1\n",
    "field_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac8fd50-a7ad-4886-8a87-4f4e1edbb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크, 학습률, 드롭아웃, 배치사이즈, 임베딩 크기 등 정의\n",
    "epochs=5\n",
    "learning_rate= 0.0001\n",
    "dropout= 0.4\n",
    "batch_size = 2048\n",
    "embed_dim= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a69717-ea07-40a4-869a-b8b1fd407042",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0720ebfd-d009-4087-b517-f63b07641745",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3db3e68-b17e-40de-a72e-647c74d50306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 70ms/step - binary_crossentropy: 0.6915 - loss: 0.6915 - val_binary_crossentropy: 0.6750 - val_loss: 0.6750\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - binary_crossentropy: 0.6709 - loss: 0.6709 - val_binary_crossentropy: 0.6268 - val_loss: 0.6268\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 77ms/step - binary_crossentropy: 0.6418 - loss: 0.6418 - val_binary_crossentropy: 0.6109 - val_loss: 0.6109\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 74ms/step - binary_crossentropy: 0.6355 - loss: 0.6355 - val_binary_crossentropy: 0.6067 - val_loss: 0.6067\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 78ms/step - binary_crossentropy: 0.6325 - loss: 0.6325 - val_binary_crossentropy: 0.6037 - val_loss: 0.6037\n"
     ]
    }
   ],
   "source": [
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29cd4d56-aae7-4ea2-af8f-23e778201557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 77246.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# 사용자에게 예측된 정보를 저장하는 딕셔너리 \n",
    "user_pred_info = {}\n",
    "# top10개\n",
    "top = 10\n",
    "# 테스트 값을 가지고 옵니다. \n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df)\n",
    "# 사용자마다 돌면서 예측 데이터 중 가장 높은 top 10만 가져옵니다. \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "# 원본 테스트 데이터에서 label이 1인 사용자 별 영화 정보를 가져옵니다.\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f59a206-afff-4506-93f9-2c90681c6f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 6760.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 53724.87it/s]\n"
     ]
    }
   ],
   "source": [
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "# 모델 예측값과 원본 테스트 데이터를 비교해서 어느정도 성능이 나왔는지 NDCG와 Hitrate를 비교합니다.\n",
    "# NDCG\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # NDCG 값 구하기\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "# Hitrate\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # hitrate 값 구하기\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    # 사용자 hitrate 결과 저장\n",
    "    mymodel_hitrate_result[user] = user_hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b34bc22-5844-4e33-8693-183668774c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65873\n",
      " mymodel hitrate :  0.6268\n"
     ]
    }
   ],
   "source": [
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb479677-f69e-469a-a017-01fb988ad862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/label_encoders_1.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('./data/field_dims_plus1.npy', field_dims)\n",
    "autoIntMLP_model.save_weights('./model/autoIntMLP_model_1.weights.h5')\n",
    "joblib.dump(label_encoders, './data/label_encoders_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd68f7a-1f1e-44da-9702-d13eefbcc3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 84ms/step - binary_crossentropy: 0.6920 - loss: 0.6920 - val_binary_crossentropy: 0.6831 - val_loss: 0.6831\n",
      "Epoch 2/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - binary_crossentropy: 0.6836 - loss: 0.6836 - val_binary_crossentropy: 0.6602 - val_loss: 0.6602\n",
      "Epoch 3/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 79ms/step - binary_crossentropy: 0.6680 - loss: 0.6680 - val_binary_crossentropy: 0.6378 - val_loss: 0.6378\n",
      "Epoch 4/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 76ms/step - binary_crossentropy: 0.6598 - loss: 0.6598 - val_binary_crossentropy: 0.6312 - val_loss: 0.6312\n",
      "Epoch 5/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 77ms/step - binary_crossentropy: 0.6568 - loss: 0.6568 - val_binary_crossentropy: 0.6280 - val_loss: 0.6280\n",
      "Epoch 6/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 77ms/step - binary_crossentropy: 0.6557 - loss: 0.6557 - val_binary_crossentropy: 0.6260 - val_loss: 0.6260\n",
      "Epoch 7/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 74ms/step - binary_crossentropy: 0.6543 - loss: 0.6543 - val_binary_crossentropy: 0.6253 - val_loss: 0.6253\n",
      "Epoch 8/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 77ms/step - binary_crossentropy: 0.6537 - loss: 0.6537 - val_binary_crossentropy: 0.6239 - val_loss: 0.6239\n",
      "Epoch 9/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 75ms/step - binary_crossentropy: 0.6527 - loss: 0.6527 - val_binary_crossentropy: 0.6253 - val_loss: 0.6253\n",
      "Epoch 10/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 74ms/step - binary_crossentropy: 0.6524 - loss: 0.6524 - val_binary_crossentropy: 0.6240 - val_loss: 0.6240\n"
     ]
    }
   ],
   "source": [
    "# 에포크, 학습률, 드롭아웃, 배치사이즈, 임베딩 크기 등 정의\n",
    "epochs=10\n",
    "learning_rate= 0.0001\n",
    "dropout= 0.6\n",
    "batch_size = 2048\n",
    "embed_dim= 16\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a3d00ad-1000-4d97-825f-91e4a3b0be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 66615.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 6493.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 51631.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65776\n",
      " mymodel hitrate :  0.62515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2fc93-4f7b-45c8-80fb-a90703d35fc0",
   "metadata": {},
   "source": [
    "에포크를 늘리고 드롭아웃 사이즈를 조정했더니, 약간 성능이 안좋아졌다.\n",
    "\n",
    "adam 말고 다른 옵티마이저들을 먼저 테스트하고, 옵티마이저를 고정시킨 후에 다른 하이퍼파라미터들을 조정해보자.\n",
    "\n",
    "옵티마이저는 AdamW, RMSprop, NAdam을 사용해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed345deb-5852-4f1e-9478-8fbdb81d631b",
   "metadata": {},
   "source": [
    "1. AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f57fa219-967f-444f-b178-a10014584aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 113ms/step - binary_crossentropy: 0.6917 - loss: 0.6917 - val_binary_crossentropy: 0.6772 - val_loss: 0.6772\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 105ms/step - binary_crossentropy: 0.6733 - loss: 0.6733 - val_binary_crossentropy: 0.6319 - val_loss: 0.6319\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 108ms/step - binary_crossentropy: 0.6437 - loss: 0.6437 - val_binary_crossentropy: 0.6133 - val_loss: 0.6133\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 111ms/step - binary_crossentropy: 0.6361 - loss: 0.6361 - val_binary_crossentropy: 0.6088 - val_loss: 0.6088\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 110ms/step - binary_crossentropy: 0.6337 - loss: 0.6337 - val_binary_crossentropy: 0.6054 - val_loss: 0.6054\n"
     ]
    }
   ],
   "source": [
    "# 에포크, 학습률, 드롭아웃, 배치사이즈, 임베딩 크기 등 정의\n",
    "epochs=5\n",
    "learning_rate= 0.0001\n",
    "dropout= 0.4\n",
    "batch_size = 2048\n",
    "embed_dim= 16\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e601470-a43f-4549-9592-78830f8c8bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 76052.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 7215.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 44701.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65909\n",
      " mymodel hitrate :  0.62665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1b0ad-e276-4f0b-8668-0dd6a879277e",
   "metadata": {},
   "source": [
    "2. RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e5a6f33-23d5-4809-884b-dec9e2c6598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 121ms/step - binary_crossentropy: 0.6893 - loss: 0.6893 - val_binary_crossentropy: 0.6830 - val_loss: 0.6830\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 99ms/step - binary_crossentropy: 0.6867 - loss: 0.6867 - val_binary_crossentropy: 0.6828 - val_loss: 0.6828\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 98ms/step - binary_crossentropy: 0.6864 - loss: 0.6864 - val_binary_crossentropy: 0.6831 - val_loss: 0.6831\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 98ms/step - binary_crossentropy: 0.6864 - loss: 0.6864 - val_binary_crossentropy: 0.6835 - val_loss: 0.6835\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 97ms/step - binary_crossentropy: 0.6866 - loss: 0.6866 - val_binary_crossentropy: 0.6832 - val_loss: 0.6832\n"
     ]
    }
   ],
   "source": [
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, momentum=0.9)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df2b2a96-e697-4bf3-ba47-f8c925214ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 69554.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:01<00:00, 5992.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 36459.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.58267\n",
      " mymodel hitrate :  0.58383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94798b7-d576-4063-a9d0-9a70d9bc1831",
   "metadata": {},
   "source": [
    "3. NAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a19ab695-d1c7-4cfc-a018-ef677986f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 105ms/step - binary_crossentropy: 0.6908 - loss: 0.6908 - val_binary_crossentropy: 0.6704 - val_loss: 0.6704\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 101ms/step - binary_crossentropy: 0.6677 - loss: 0.6677 - val_binary_crossentropy: 0.6262 - val_loss: 0.6262\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 96ms/step - binary_crossentropy: 0.6411 - loss: 0.6411 - val_binary_crossentropy: 0.6117 - val_loss: 0.6117\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 95ms/step - binary_crossentropy: 0.6354 - loss: 0.6354 - val_binary_crossentropy: 0.6067 - val_loss: 0.6067\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 94ms/step - binary_crossentropy: 0.6333 - loss: 0.6333 - val_binary_crossentropy: 0.6048 - val_loss: 0.6048\n"
     ]
    }
   ],
   "source": [
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92e3eca2-bbc8-4445-b835-b773ba4ac8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 64771.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 6885.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 45211.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65875\n",
      " mymodel hitrate :  0.62645\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87eb96-b432-42ca-935d-6cfc06edfd39",
   "metadata": {},
   "source": [
    "4. NAdam with learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e75fa0d2-1062-484d-925a-e11041dc1d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 126ms/step - binary_crossentropy: 0.6914 - loss: 0.6914 - val_binary_crossentropy: 0.6768 - val_loss: 0.6768\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - binary_crossentropy: 0.6754 - loss: 0.6754 - val_binary_crossentropy: 0.6449 - val_loss: 0.6449\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - binary_crossentropy: 0.6519 - loss: 0.6519 - val_binary_crossentropy: 0.6222 - val_loss: 0.6222\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - binary_crossentropy: 0.6405 - loss: 0.6405 - val_binary_crossentropy: 0.6134 - val_loss: 0.6134\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - binary_crossentropy: 0.6370 - loss: 0.6370 - val_binary_crossentropy: 0.6095 - val_loss: 0.6095\n"
     ]
    }
   ],
   "source": [
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=lr_schedule, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9a957c3-cd61-4c49-b898-39a1a6c2f00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 67611.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 6762.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 46909.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65909\n",
      " mymodel hitrate :  0.62726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932299a-ba74-40da-ba65-7a0c817d060b",
   "metadata": {},
   "source": [
    "5. AdamW with learning rate scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a77a332-b243-4efe-82ee-91552761e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 102ms/step - binary_crossentropy: 0.6915 - loss: 0.6915 - val_binary_crossentropy: 0.6768 - val_loss: 0.6768\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - binary_crossentropy: 0.6741 - loss: 0.6741 - val_binary_crossentropy: 0.6392 - val_loss: 0.6392\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - binary_crossentropy: 0.6487 - loss: 0.6487 - val_binary_crossentropy: 0.6179 - val_loss: 0.6179\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - binary_crossentropy: 0.6384 - loss: 0.6384 - val_binary_crossentropy: 0.6113 - val_loss: 0.6113\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - binary_crossentropy: 0.6350 - loss: 0.6350 - val_binary_crossentropy: 0.6080 - val_loss: 0.6080\n"
     ]
    }
   ],
   "source": [
    "embed_dim= 16\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aeaab98d-2e62-4b35-aaa0-129fc24d4313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 69236.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 6440.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 43471.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65916\n",
      " mymodel hitrate :  0.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21101107-de3c-4e64-97b6-866df7562fee",
   "metadata": {},
   "source": [
    "테스트 결과 NAdam이나 AdamW를 사용하면서 learning rate을 시간에 따라 줄이는 learning_rate scheduling을 사용하는 것이 성능이 가장 좋았다. 이를 바탕으로 하이퍼 파라미터를 튜닝해보자.\n",
    "\n",
    "optimizer는 adamw로 고정하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fffdea9-48f5-45c6-bc9f-034dd9c8a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 149ms/step - binary_crossentropy: 0.6902 - loss: 0.6902 - val_binary_crossentropy: 0.6678 - val_loss: 0.6678\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 137ms/step - binary_crossentropy: 0.6640 - loss: 0.6640 - val_binary_crossentropy: 0.6256 - val_loss: 0.6256\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 138ms/step - binary_crossentropy: 0.6404 - loss: 0.6404 - val_binary_crossentropy: 0.6140 - val_loss: 0.6140\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 139ms/step - binary_crossentropy: 0.6350 - loss: 0.6350 - val_binary_crossentropy: 0.6093 - val_loss: 0.6093\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 141ms/step - binary_crossentropy: 0.6326 - loss: 0.6326 - val_binary_crossentropy: 0.6066 - val_loss: 0.6066\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "dropout= 0.4\n",
    "batch_size = 2048\n",
    "embed_dim= 32\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aeebb985-63a6-4504-9c6c-76c87b00b43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 53247.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:01<00:00, 5257.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 45674.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65848\n",
      " mymodel hitrate :  0.62627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530ecd8-8dac-4cff-993a-321816dc1dfc",
   "metadata": {},
   "source": [
    "embed dimension을 늘렸더니 미세하게 성능이 안좋아진 모습이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ea63d7e-a730-4b68-9c34-1d0e50787873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 108ms/step - binary_crossentropy: 0.6916 - loss: 0.6916 - val_binary_crossentropy: 0.6772 - val_loss: 0.6772\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 104ms/step - binary_crossentropy: 0.6738 - loss: 0.6738 - val_binary_crossentropy: 0.6450 - val_loss: 0.6450\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 103ms/step - binary_crossentropy: 0.6464 - loss: 0.6464 - val_binary_crossentropy: 0.6200 - val_loss: 0.6200\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 102ms/step - binary_crossentropy: 0.6284 - loss: 0.6284 - val_binary_crossentropy: 0.6091 - val_loss: 0.6091\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 104ms/step - binary_crossentropy: 0.6213 - loss: 0.6213 - val_binary_crossentropy: 0.6042 - val_loss: 0.6042\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "dropout= 0.2\n",
    "batch_size = 2048\n",
    "embed_dim= 16\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=50,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5747e920-73e8-49c7-af6c-cafc83628c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 54974.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:01<00:00, 4890.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 25570.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65881\n",
      " mymodel hitrate :  0.6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d3ffcc6-8255-48d1-8864-5d5f74a59d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 150ms/step - binary_crossentropy: 0.6906 - loss: 0.6906 - val_binary_crossentropy: 0.6674 - val_loss: 0.6674\n",
      "Epoch 2/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 144ms/step - binary_crossentropy: 0.6618 - loss: 0.6618 - val_binary_crossentropy: 0.6189 - val_loss: 0.6189\n",
      "Epoch 3/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 145ms/step - binary_crossentropy: 0.6318 - loss: 0.6318 - val_binary_crossentropy: 0.6055 - val_loss: 0.6055\n",
      "Epoch 4/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 138ms/step - binary_crossentropy: 0.6244 - loss: 0.6244 - val_binary_crossentropy: 0.6016 - val_loss: 0.6016\n",
      "Epoch 5/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 137ms/step - binary_crossentropy: 0.6220 - loss: 0.6220 - val_binary_crossentropy: 0.5998 - val_loss: 0.5998\n",
      "Epoch 6/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 140ms/step - binary_crossentropy: 0.6205 - loss: 0.6205 - val_binary_crossentropy: 0.5981 - val_loss: 0.5981\n",
      "Epoch 7/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 137ms/step - binary_crossentropy: 0.6207 - loss: 0.6207 - val_binary_crossentropy: 0.5970 - val_loss: 0.5970\n",
      "Epoch 8/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 135ms/step - binary_crossentropy: 0.6185 - loss: 0.6185 - val_binary_crossentropy: 0.5972 - val_loss: 0.5972\n",
      "Epoch 9/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 135ms/step - binary_crossentropy: 0.6182 - loss: 0.6182 - val_binary_crossentropy: 0.5962 - val_loss: 0.5962\n",
      "Epoch 10/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 135ms/step - binary_crossentropy: 0.6182 - loss: 0.6182 - val_binary_crossentropy: 0.5960 - val_loss: 0.5960\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "dropout= 0.3\n",
    "batch_size = 2048\n",
    "embed_dim= 32\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8b607e6-a74c-46f4-906f-de18a4a36c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 77556.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 7601.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 45674.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65881\n",
      " mymodel hitrate :  0.62667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fed3353a-8047-482c-a40f-e23717a6b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 102ms/step - binary_crossentropy: 0.6917 - loss: 0.6919 - val_binary_crossentropy: 0.6793 - val_loss: 0.6808\n",
      "Epoch 2/7\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 106ms/step - binary_crossentropy: 0.6787 - loss: 0.6809 - val_binary_crossentropy: 0.6550 - val_loss: 0.6602\n",
      "Epoch 3/7\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 105ms/step - binary_crossentropy: 0.6554 - loss: 0.6615 - val_binary_crossentropy: 0.6265 - val_loss: 0.6351\n",
      "Epoch 4/7\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 106ms/step - binary_crossentropy: 0.6367 - loss: 0.6457 - val_binary_crossentropy: 0.6154 - val_loss: 0.6250\n",
      "Epoch 5/7\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 106ms/step - binary_crossentropy: 0.6325 - loss: 0.6420 - val_binary_crossentropy: 0.6114 - val_loss: 0.6210\n",
      "Epoch 6/7\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 105ms/step - binary_crossentropy: 0.6298 - loss: 0.6393 - val_binary_crossentropy: 0.6092 - val_loss: 0.6186\n",
      "Epoch 7/7\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 107ms/step - binary_crossentropy: 0.6279 - loss: 0.6373 - val_binary_crossentropy: 0.6072 - val_loss: 0.6165\n"
     ]
    }
   ],
   "source": [
    "epochs= 7\n",
    "dropout= 0.3\n",
    "batch_size = 2048\n",
    "embed_dim= 16\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0.001, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e631e674-00a0-4cb9-85a5-bbda15ff5488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 65430.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:01<00:00, 5605.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 35690.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65948\n",
      " mymodel hitrate :  0.62753\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b065ac44-a845-4236-9f43-205c7158873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 159ms/step - binary_crossentropy: 0.6914 - loss: 0.6916 - val_binary_crossentropy: 0.6776 - val_loss: 0.6793\n",
      "Epoch 2/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 152ms/step - binary_crossentropy: 0.6740 - loss: 0.6772 - val_binary_crossentropy: 0.6409 - val_loss: 0.6485\n",
      "Epoch 3/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 151ms/step - binary_crossentropy: 0.6429 - loss: 0.6516 - val_binary_crossentropy: 0.6171 - val_loss: 0.6268\n",
      "Epoch 4/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 148ms/step - binary_crossentropy: 0.6320 - loss: 0.6417 - val_binary_crossentropy: 0.6117 - val_loss: 0.6210\n",
      "Epoch 5/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 146ms/step - binary_crossentropy: 0.6277 - loss: 0.6370 - val_binary_crossentropy: 0.6088 - val_loss: 0.6178\n",
      "Epoch 6/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 146ms/step - binary_crossentropy: 0.6269 - loss: 0.6359 - val_binary_crossentropy: 0.6062 - val_loss: 0.6150\n",
      "Epoch 7/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - binary_crossentropy: 0.6251 - loss: 0.6339 - val_binary_crossentropy: 0.6049 - val_loss: 0.6136\n",
      "Epoch 8/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 149ms/step - binary_crossentropy: 0.6237 - loss: 0.6324 - val_binary_crossentropy: 0.6036 - val_loss: 0.6122\n",
      "Epoch 9/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 150ms/step - binary_crossentropy: 0.6233 - loss: 0.6319 - val_binary_crossentropy: 0.6031 - val_loss: 0.6115\n",
      "Epoch 10/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 143ms/step - binary_crossentropy: 0.6223 - loss: 0.6307 - val_binary_crossentropy: 0.6028 - val_loss: 0.6112\n"
     ]
    }
   ],
   "source": [
    "epochs= 10\n",
    "dropout= 0.3\n",
    "batch_size = 2048\n",
    "embed_dim= 32\n",
    "\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims, \n",
    "    embedding_size=embed_dim, \n",
    "    att_layer_num=3, \n",
    "    att_head_num=2, \n",
    "    att_res=True, \n",
    "    dnn_hidden_units=(32, 32), \n",
    "    dnn_activation='relu',\n",
    "    l2_reg_dnn=0.001, \n",
    "    l2_reg_embedding=1e-5, \n",
    "    dnn_use_bn=False, \n",
    "    dnn_dropout=dropout, \n",
    "    init_std=0.0001\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-5)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "\n",
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d08c5de8-661f-4c3e-8c91-948eea81d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\지우\\AppData\\Local\\Temp\\ipykernel_379640\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6038/6038 [00:00<00:00, 58319.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5994/5994 [00:01<00:00, 5776.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5994/5994 [00:00<00:00, 50011.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.65974\n",
      " mymodel hitrate :  0.62713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_pred_info = {}\n",
    "top = 10\n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df) \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_hitrate_result[user] = user_hitrate\n",
    "\n",
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2d7b1-805c-42c4-b24e-9685a884e7d3",
   "metadata": {},
   "source": [
    "현재까지의 결과를 정리해보면 다음과 같다. ndcg와 hitrate를 기준으로 성능을 평가하고 있다.\n",
    "\n",
    "1. 노드 상의 코드를 이용한 AutoInt+ 모델\n",
    " ndcg :  0.65909, hitrate :  0.626\n",
    "2. AdamW 적용\n",
    " ndcg : 0.65916, hitrate : 0.6271\n",
    "3. epoch 7, dropout 0.3, l2_reg_dnn 0.001\n",
    " ndcg : 0.65948, hitrate : 0.62753\n",
    "4. epoch 10, dropout 0.3, l2_reg_dnn 0.001, embed_dim 32\n",
    " ndcg : 0.65974, hitrate : 0.62713\n",
    "\n",
    "3과 4는 성능이 비슷하지만, ndcg가 가장 높게 나온 4번을 기준으로 streamlit 시각화를 위한 모델을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3247b1af-aaed-4117-9707-0bad484e9dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/label_encoders_tuned.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('./data/field_dims_tuned.npy', field_dims)\n",
    "autoIntMLP_model.save_weights('./model/autoIntMLP_model_tuned.weights.h5')\n",
    "joblib.dump(label_encoders, './data/label_encoders_tuned.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410781e-10eb-4224-b19b-9c78ee6610dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 회고\n",
    "\n",
    "사실 아직도 완전히 이 개념을 이해하지는 못한 것 같다.\n",
    "\n",
    "하지만 분명 논문과 코드 리뷰를 통해서 공부한 것보다는 훨씬 잘 이해된 것 같다.\n",
    "\n",
    "오토인트 모델이 attention 메커니즘을 사용한다는 점, 어텐션 메커니즘을 q, k, v값에 적용하고 여러 개를 함께 묶어 값을 도출한다는 것을 좀 더 직관적으로 알 수 있었다.\n",
    "\n",
    "또한, 오토인트+ 모델은 거기에 dnn을 병렬적으로 배치함으로써 성능의 상승을 꾀했다.\n",
    "\n",
    "사실 실제로 이런 저런 테스트를 진행해보면서 느낀 것은, (그리고 영상에서도 해당 부분을 언급했던 것으로 기억하는데) 플러스 모델과 일반 모델이 그리 큰 성능 차이를 보여주지 못한다는 점이다.\n",
    "\n",
    "또한, 추천 시스템이 원래 이런 것인지, 아니면 내가 건드려볼 수 있는 더 다양한 하이퍼 파라미터들을 내버려둬서 그런 것인지는 모르겠지만, 파라미터를 조절한다고 크게 성능의 향상이나 저하가 있지는 않았다. 안정적인 모델이라는 반증일수도 있겠다.\n",
    "\n",
    "그리고 파이토치와 텐서플로를 비교하고 텐서플로를 통해 파이토치 코드를 변환시켜 적용해봄으로써 딥러닝 모델의 패키지별 구성이 어떤 식으로 일어나는지 알 수 있었다.\n",
    "\n",
    "어떤 부분에서는 파이토치가 좀 더 편한 부분이 있는 것도 같아, 파이토치도 한 번 배워놓으면 좋겠다는 생각도 든다.\n",
    "\n",
    "우선 여러 테스트를 통해 기본 +모델보다 좀 더 높은 성능을 보이는 + 튜닝 모델을 만들어 냈다는 것에 주안점을 두고,\n",
    "\n",
    "streamlit으로도 무사히 해당 기능을 구현해냈다는 데 만족스럽다.\n",
    "\n",
    "또한 streamlit을 일종의 rag 플랫폼이라고 생각했었는데, 이런 식으로 MLOps를 위한 패키지로도 사용할 수 있다는 점이 신선했다.\n",
    "\n",
    "마지막으로 굉장히 재밌다고 느꼈던 점이 있다.\n",
    "\n",
    "streamlit 결과를 봤을 때 처음에는 성능이 좋지 않다고 생각했다. 추천 결과와 이력 상의 값들이 생각보다 너무 맞지 않다고 느꼈기 때문이다.\n",
    "\n",
    "하지만 영화 하나하나를 잘 따져보면, 생각보다 영화의 느낌들이 비슷한 것을 볼 수 있다.\n",
    "\n",
    "AutoInt는 사실 이전의 추천시스템들이 너무 one-order나 two-order, high-order에 선형적으로 영향을 받는 것을 문제삼아 등장한 모델이기 때문에, 더 feature간 interaction을 중요하게 모델링한다고 했다.\n",
    "\n",
    "따라서, 하나의 데이터의 '장르'라는 feature에만 영향을 받지 않고, 그것보다 훨씬 더 다양한 측면에서 영향을 받는 것이다.\n",
    "\n",
    "그렇기 때문에 그냥 눈으로 봐도 생각보다 비슷한 영화들이 추천된 것을 확인할 수 있었다. 장르와 같은 직관적인 부분 말고도 다른 특성들을 통해 이런 추천을 수행할 수 있다는 게 확실히 신기하긴 했다.\n",
    "\n",
    "추천 시스템이 재미있는 점 중 하나는, 직관적으로 보이는 ndcg나 hitrate와 같은 성능 수치가 실제 결괏값에 미치는 영향도와 다를 수 있다는 점이다.\n",
    "\n",
    "사실 수많은 영화들이 존재하고, 수많은 상품들이 존재하는데 어떤 사람이 어떤 상품을 좋아한다고 이미 말했다고 해서 추천된 상품을 좋아하지 않을 거라는 보장이 없지 않은가?\n",
    "\n",
    "이런 정성적인 부분을 어떻게 고려하느냐도 추천 시스템을 개발하는 데 어느 정도 도움이 되지 않을까 생각하면서, 이 프로젝트를 마무리하겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
